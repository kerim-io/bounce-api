# Architecture

## Design Philosophy

The Pydantic LLM Mixin follows the **"Trust the LLM" philosophy** from Mari-OS:

1. **Minimal prompting** - Field descriptions ARE the spec, LLM infers everything from schema
2. **Schema-first** - Pydantic model defines structure, LLM generates content
3. **Fail loudly** - Full tracebacks, no silent failures
4. **One execution path** - No fallbacks, alternatives, or legacy branches

## Core Components

### GenerativeMixin

The heart of the package. Provides `generate_instance()` method to any Pydantic model.

**Key features:**
- Multi-strategy JSON extraction (4 fallback strategies)
- Robust retry logic with exponential backoff
- System-managed field filtering
- Custom validation callbacks
- Streaming support

**JSON Extraction Strategies:**

1. **Regex pattern** - Standard ````json\n{...}\n``` markdown blocks
2. **Manual parsing** - Handle nested backticks and malformed blocks
3. **Brace matching** - Find complete `{...}` blocks with proper nesting
4. **Raw JSON** - Try parsing entire response as JSON

Each strategy validates JSON before returning. Falls through to next on failure.

### GroqClient

Async HTTP client for Groq API with:
- **Rate limiting** - Token bucket algorithm (300 req/min, 144k req/day)
- **Retry logic** - Exponential backoff on transient errors
- **Model validation** - Validates reasoning parameters per model
- **Event loop safety** - Recreates client on loop change

**Retry behavior:**
- Transient LLM errors (503, rate limits, tool_choice): Exponential backoff
- Network errors: Exponential backoff (max 30s)
- Empty responses: Treat as rate limit, backoff
- Non-transient errors: Fail immediately

### Factory Layer

Provider abstraction with unified request/response models:

```python
ChatRequest → Provider-specific request → API
API response → Provider-specific response → ChatResponse
```

This allows easy addition of new providers (Claude, OpenAI, etc.) without changing GenerativeMixin.

### Prompts

Centralized prompt templates following minimal prompting philosophy:

```python
SCHEMA_SYSTEM_MESSAGE_TEMPLATE = """Today's date: {current_date}

Respond with JSON code block:
```json
{{response matching schema}}
```

Schema:
{schema_json}
"""
```

No verbose instructions - schema is sufficient, trust the LLM.

## Data Flow

### Instance Generation Flow

1. **User calls** `Model.generate_instance(client, conversation_history)`
2. **Schema generation** - `_generate_prompt()` creates system message with JSON schema
3. **History trimming** - `_select_recent_exchanges()` keeps last 3-5 exchanges
4. **Message assembly** - System message + trimmed history → ChatRequest
5. **LLM call** - Factory routes to provider (Groq)
6. **Response extraction** - Multi-strategy JSON extraction
7. **Validation** - Pydantic validation + optional custom validation
8. **Return** - Validated model instance

### Retry Flow

```
Attempt → LLM API call → Success? → Return
                      ↓ No
                   Transient error? → Exponential backoff → Retry
                      ↓ No
                   JSON/validation error? → Base delay → Retry
                      ↓ No
                   Fail with traceback
```

Max retries configurable (default: 3)

## System-Managed Fields

Fields marked with `json_schema_extra={"system_managed": True}` are:
- **Excluded** from LLM schema via `model_json_schema()` override
- **Auto-generated** on instance creation (id, created_at, etc.)
- **Never** generated by LLM

This keeps LLM schemas clean while preserving full Pydantic functionality.

## Conversation History Management

**Exchange** = user message + assistant response

**Trimming logic:**
- Min exchanges: 3 (6 messages)
- Max exchanges: 5 (10 messages)
- Preserves complete exchanges (never cuts mid-conversation)

**Format:**
```python
[
    {"role": "system", "content": "{schema}"},      # Schema
    {"role": "user", "content": "..."},             # Exchange 1
    {"role": "assistant", "content": "..."},
    {"role": "user", "content": "..."},             # Exchange 2
    {"role": "assistant", "content": "..."},
    {"role": "user", "content": "{current}"}        # Current query
]
```

## Error Handling

### Error Classification

1. **Transient LLM errors** - Retry with exponential backoff
   - Rate limits (429)
   - Server errors (503, 502)
   - tool_choice errors
   - Empty responses

2. **Parsing errors** - Retry with base delay
   - JSON extraction failure
   - JSON decode error
   - Pydantic validation error

3. **Non-transient errors** - Fail immediately
   - Authentication errors
   - Invalid model
   - Network unreachable

### Logging

All errors logged with full context:
- Exception type and message
- LLM response excerpt (first/last 500 chars)
- Full traceback via `traceback.print_exc()`

No silent failures - everything visible for debugging.

## Rate Limiting

**Token bucket algorithm** with two buckets:
- **Minute bucket** - 300 tokens, refills every 60s
- **Day bucket** - 144,000 tokens, refills every 24h

**Backoff calculation:**
```python
delay = min(exponential_base ** consecutive_failures, max_backoff)
```

Default: base=2.0, max=60s

**Adaptive behavior:**
- Success → Reset backoff counter
- Rate limit → Increment counter, backoff
- Multiple failures → Exponential increase (2s, 4s, 8s, 16s, 32s, 60s)

## Extension Points

### Adding New Providers

1. Implement provider client in `providers/your_provider/`
2. Add to factory.py routing
3. Update ChatRequest/ChatResponse conversion
4. Add tests

### Custom Validation

Two levels:
1. **Pydantic validators** - Schema-level constraints
2. **validation_callback** - Business logic after Pydantic validation

Example:
```python
def validate_length(instance: Model) -> tuple[bool, str | None]:
    if len(instance.items) != 10:
        return False, f"Need 10 items, got {len(instance.items)}"
    return True, None
```

### Streaming Callbacks

Receive events during generation:
- `("raw_llm_response", {"model": str, "response": str})`
- `("parsed_instance", {"model": str, "instance": dict})`

Useful for:
- Real-time UI updates
- Logging/monitoring
- Debugging LLM behavior

## Performance Considerations

### Token Usage

- **System message** - ~200-500 tokens (schema size dependent)
- **History** - 3-5 exchanges (varies by length)
- **Response** - Model-dependent (typically 500-2000 tokens)

Total: ~1000-3000 tokens per generation

### Latency

- **LLM API** - 1-3 seconds (Groq is fast)
- **JSON extraction** - <10ms (multi-strategy is efficient)
- **Pydantic validation** - <1ms (schema validation is fast)

Total: ~1-3 seconds per generation

### Rate Limits

Groq Dev tier:
- 300 requests/minute
- 144,000 requests/day
- 60,000 tokens/minute

Built-in rate limiting prevents hitting these limits.

## Testing Strategy

Tests demonstrate:
1. **Basic generation** - Simple instance from conversation
2. **Conversation history** - Multi-turn conversations
3. **Custom validation** - Business logic constraints
4. **Streaming** - Callback events
5. **JSON extraction** - All 4 strategies
6. **System fields** - Schema filtering
7. **Error handling** - Invalid inputs

All tests use real Groq API (no mocks - trust real behavior).

## Future Enhancements

Potential additions:
- Claude provider support
- OpenAI provider support
- Streaming LLM responses (token-by-token)
- Caching layer for repeated schemas
- Prompt style templates
- Context window management (summarization)
